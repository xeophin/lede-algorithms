{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lede Algorithms 2018 Week 2 class 1 - Text analysis\n",
    "\n",
    "Before running this notebook you will need to install a few things:\n",
    "\n",
    "```\n",
    "pip3 install textblob\n",
    "python3 -m textblob.download_corpora\n",
    "pip3 install scipy\n",
    "pip3 install scikit-learn\n",
    "```\n",
    "\n",
    "For more text analysis goodness, check out Jonathan Soma's 2017 notebooks:\n",
    "- [TextBlob spaCy sklearn lemmas stems and vectorization](http://jonathansoma.com/lede/algorithms-2017/classes/text-analysis/textblob-spacy-sklearn-lemmas-stems-and-vectorization/)\n",
    "- [Counting and Stemming](http://jonathansoma.com/lede/algorithms-2017/classes/more-text-analysis/counting-and-stemming/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the packages we will be using\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load some press releases to play around with. These were scraped from NJ Senator Menendez' site in 2012."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1530"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr = pd.read_csv('menendez-press-releases.csv')\n",
    "len(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://menendez.senate.gov/newsroom/press/rele...</td>\n",
       "      <td>Menendez Statement on Black History Month\\n   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://menendez.senate.gov/newsroom/press/rele...</td>\n",
       "      <td>Menendez Praises Susan G. Komen For Reversing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://menendez.senate.gov/newsroom/press/rele...</td>\n",
       "      <td>Menendez Applauds Dentists’ Pro-Bono Work For ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://menendez.senate.gov/newsroom/press/rele...</td>\n",
       "      <td>Senator Menendez Applauds Passage of STOCK Act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://menendez.senate.gov/newsroom/press/rele...</td>\n",
       "      <td>Menendez Hails Banking Committee Passage of Ir...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  http://menendez.senate.gov/newsroom/press/rele...   \n",
       "1  http://menendez.senate.gov/newsroom/press/rele...   \n",
       "2  http://menendez.senate.gov/newsroom/press/rele...   \n",
       "3  http://menendez.senate.gov/newsroom/press/rele...   \n",
       "4  http://menendez.senate.gov/newsroom/press/rele...   \n",
       "\n",
       "                                                text  \n",
       "0  Menendez Statement on Black History Month\\n   ...  \n",
       "1  Menendez Praises Susan G. Komen For Reversing ...  \n",
       "2  Menendez Applauds Dentists’ Pro-Bono Work For ...  \n",
       "3  Senator Menendez Applauds Passage of STOCK Act...  \n",
       "4  Menendez Hails Banking Committee Passage of Ir...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "These press releases are on all sorts of topics. Take a look at a few, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Senator Menendez Slams Unfair Imprisonment of Former Ukrainian Prime Minister Yulia Tymoshenko\n",
      "                    \n",
      "                      February 1, 2012\n",
      "                     WASHINGTON – United States Senator Robert Menendez (D-NJ) participated in the Senate Foreign Relations Committee hearing on Ukraine today, giving him the opportunity to meet Eugenia Tymoshenko and to discuss the inhumane detention of her mother, the former Prime Minister Yulia Tymoshenko.  Last October, a Ukrainian court sentenced Yulia Tymoshenko to seven years in prison after she was found guilty of abuse of office when brokering a 2009 gas deal with Russia.  The Senator expressed his sympathy and support to Ms. Tymoshenko and vowed to assist in her efforts to have her mother freed from prison.\n",
      "\n",
      " “Your mother is a pioneering and incredibly strong woman,” the Senator told the younger Tymoshenko. “Yulia is an example for all people who care so much about their country that they are willing to endure extraordinary hardship and not just lay down in the face of oppression and cruelty. I think this hearing is a wonderful way to inform the American people not only about your mother, but the other opposition leaders in jail and to keep the pressure on the Ukrainian authorities to give Yulia her freedom back.”\n",
      " “Every day the human rights situation in Ukraine worsens and it’s starting to remind me of the shameful conditions of the Soviet era.  Yulia Tymoshenko worked very hard to throw off the tyranny of the Soviet past, and to see her and other opposition leaders in jail cells is a reminder of how much work remains to be done to improve human rights and political freedom in today’s Ukraine.  We must convince President Yanukovych that Ukraine’s path to freedom and economic prosperity is not through Soviet-style centralized government, but by way of releasing the power, intelligence and dignity of the Ukrainian people while respecting the rights of all its citizens,” added Menendez.\n",
      "\n",
      "\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "print(pr.text[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are press releases about government programs, holidays, foreign policy, and more. Can an algorithm tell us something about which topics there are?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Sentences and tokens\n",
    "\n",
    "The first step of text analysis is typically breaking the text into sentences and words or more accurately, \"tokens\" which are basically words but can also be punctuation and numbers.\n",
    "\n",
    "We'll use the TextBlob package, which has many easy and useful text processing methods -- though as we will see it's also kind of dumb in many cases.\n",
    "\n",
    "Let's start by trying analyze the sentences of the first press release in the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"Menendez and Lautenberg Applaud USDOT’s $4.3 Million Award to National Transit Institute at Rutgers\n",
       "                     \n",
       "                             Funding will provide job training and education for public transportation workers\n",
       "                     \n",
       "                       August 3, 2011\n",
       "                      WASHINGTON, D.C. – Today, U.S.\"),\n",
       " Sentence(\"Senators Robert Menendez (D-NJ) and Frank R. Lautenberg (D-NJ) applauded Secretary Ray LaHood on a $4.3 million grant from the U.S. Department of Transportation in support of the National Transit Institute (NTI) at Rutgers.\"),\n",
       " Sentence(\"Since 1991, NTI has served as the premiere research, training, and educational institute in the country dedicated to public transportation.\"),\n",
       " Sentence(\"This award will enable critical enhancements to NTI’s ongoing work, including safety and security training, procurement, planning and advanced technologies.\"),\n",
       " Sentence(\"“Transit is a critical element of our transportation network and recognition of its importance continues to rise,” said Menendez.\"),\n",
       " Sentence(\"“Today, with gas prices around $4 a gallon and oil companies reaping record profits, with the threat of climate change and growing wealth disparity, transit is part of the solution for a number of interconnected challenges.\"),\n",
       " Sentence(\"NTI’s work is essential for the industry to continue to create good, long term jobs, provide families with access to opportunity, and help our communities grow in ways that are smart and efficient.”\n",
       " \"Millions of people count on efficient and reliable public transportation to get to and from their homes and their jobs, and transit workers make this possible,\" said Senator Lautenberg.\"),\n",
       " Sentence(\"\"NTI at Rutgers provides critical training for transit operators around the country and helps ensure the best management, safety and security of our public transportation systems.\"),\n",
       " Sentence(\"This federal grant makes an investment in NTI so that it can continue its work to keep Americans on the move.\"\"),\n",
       " Sentence(\"“This grant will ensure that the existing and importantly, the new generation of public transportation workers receive the training that they need to comply with federal regulations and operate safe and efficient transit services,” said NTI’s director, Paul Larrousse.NTI works cooperatively through partnerships with industry, government, institutions, and associations to develop high quality programs that build careers and help make our communities healthier and more livable.\"),\n",
       " Sentence(\"###\")]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "press_release_text = pr.text[212]\n",
    "doc = TextBlob(press_release_text)\n",
    "doc.sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, TextBlob is a little naive about what counts as a sentence. Or perhaps the problem is that real text contains lots of things that aren't really sentences. What should we do with the title and the dateline? Even so, it seems to have problems with quotes and newlines.\n",
    "\n",
    "Anyway, let's take one of these sentences and play with it further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence(\"“This grant will ensure that the existing and importantly, the new generation of public transportation workers receive the training that they need to comply with federal regulations and operate safe and efficient transit services,” said NTI’s director, Paul Larrousse.NTI works cooperatively through partnerships with industry, government, institutions, and associations to develop high quality programs that build careers and help make our communities healthier and more livable.\")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = doc.sentences[9]\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `Sentence` object acts just like a Python string. Let's try to break it into words for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['“This', 'grant', 'will', 'ensure', 'that', 'the', 'existing', 'and', 'importantly,', 'the', 'new', 'generation', 'of', 'public', 'transportation', 'workers', 'receive', 'the', 'training', 'that', 'they', 'need', 'to', 'comply', 'with', 'federal', 'regulations', 'and', 'operate', 'safe', 'and', 'efficient', 'transit', 'services,”', 'said', 'NTI’s', 'director,', 'Paul', 'Larrousse.NTI', 'works', 'cooperatively', 'through', 'partnerships', 'with', 'industry,', 'government,', 'institutions,', 'and', 'associations', 'to', 'develop', 'high', 'quality', 'programs', 'that', 'build', 'careers', 'and', 'help', 'make', 'our', 'communities', 'healthier', 'and', 'more', 'livable.'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that all of the punctuation and capitalization is still there. If we're counting occurences of the word \"nation\" we will miss \"Nation’s\". We need a smarter way to extract words. This process is called tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['“', 'This', 'grant', 'will', 'ensure', 'that', 'the', 'existing', 'and', 'importantly', ',', 'the', 'new', 'generation', 'of', 'public', 'transportation', 'workers', 'receive', 'the', 'training', 'that', 'they', 'need', 'to', 'comply', 'with', 'federal', 'regulations', 'and', 'operate', 'safe', 'and', 'efficient', 'transit', 'services', ',', '”', 'said', 'NTI', '’', 's', 'director', ',', 'Paul', 'Larrousse.NTI', 'works', 'cooperatively', 'through', 'partnerships', 'with', 'industry', ',', 'government', ',', 'institutions', ',', 'and', 'associations', 'to', 'develop', 'high', 'quality', 'programs', 'that', 'build', 'careers', 'and', 'help', 'make', 'our', 'communities', 'healthier', 'and', 'more', 'livable', '.'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what about different forms of the same word? Suppose we want to count \"education\" and \"educate\" as the same thing? This is where _lemmatization_ and _stemming_ come in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL: “ | LEMMA: “ | STEM: “\n",
      "ORIGINAL: This | LEMMA: This | STEM: thi\n",
      "ORIGINAL: grant | LEMMA: grant | STEM: grant\n",
      "ORIGINAL: will | LEMMA: will | STEM: will\n",
      "ORIGINAL: ensure | LEMMA: ensure | STEM: ensur\n",
      "ORIGINAL: that | LEMMA: that | STEM: that\n",
      "ORIGINAL: the | LEMMA: the | STEM: the\n",
      "ORIGINAL: existing | LEMMA: existing | STEM: exist\n",
      "ORIGINAL: and | LEMMA: and | STEM: and\n",
      "ORIGINAL: importantly | LEMMA: importantly | STEM: importantli\n",
      "ORIGINAL: the | LEMMA: the | STEM: the\n",
      "ORIGINAL: new | LEMMA: new | STEM: new\n",
      "ORIGINAL: generation | LEMMA: generation | STEM: gener\n",
      "ORIGINAL: of | LEMMA: of | STEM: of\n",
      "ORIGINAL: public | LEMMA: public | STEM: public\n",
      "ORIGINAL: transportation | LEMMA: transportation | STEM: transport\n",
      "ORIGINAL: workers | LEMMA: worker | STEM: worker\n",
      "ORIGINAL: receive | LEMMA: receive | STEM: receiv\n",
      "ORIGINAL: the | LEMMA: the | STEM: the\n",
      "ORIGINAL: training | LEMMA: training | STEM: train\n",
      "ORIGINAL: that | LEMMA: that | STEM: that\n",
      "ORIGINAL: they | LEMMA: they | STEM: they\n",
      "ORIGINAL: need | LEMMA: need | STEM: need\n",
      "ORIGINAL: to | LEMMA: to | STEM: to\n",
      "ORIGINAL: comply | LEMMA: comply | STEM: compli\n",
      "ORIGINAL: with | LEMMA: with | STEM: with\n",
      "ORIGINAL: federal | LEMMA: federal | STEM: feder\n",
      "ORIGINAL: regulations | LEMMA: regulation | STEM: regul\n",
      "ORIGINAL: and | LEMMA: and | STEM: and\n",
      "ORIGINAL: operate | LEMMA: operate | STEM: oper\n",
      "ORIGINAL: safe | LEMMA: safe | STEM: safe\n",
      "ORIGINAL: and | LEMMA: and | STEM: and\n",
      "ORIGINAL: efficient | LEMMA: efficient | STEM: effici\n",
      "ORIGINAL: transit | LEMMA: transit | STEM: transit\n",
      "ORIGINAL: services | LEMMA: service | STEM: servic\n",
      "ORIGINAL: ” | LEMMA: ” | STEM: ”\n",
      "ORIGINAL: said | LEMMA: said | STEM: said\n",
      "ORIGINAL: NTI | LEMMA: NTI | STEM: nti\n",
      "ORIGINAL: ’ | LEMMA: ’ | STEM: ’\n",
      "ORIGINAL: s | LEMMA: s | STEM: s\n",
      "ORIGINAL: director | LEMMA: director | STEM: director\n",
      "ORIGINAL: Paul | LEMMA: Paul | STEM: paul\n",
      "ORIGINAL: Larrousse.NTI | LEMMA: Larrousse.NTI | STEM: larrousse.nti\n",
      "ORIGINAL: works | LEMMA: work | STEM: work\n",
      "ORIGINAL: cooperatively | LEMMA: cooperatively | STEM: cooper\n",
      "ORIGINAL: through | LEMMA: through | STEM: through\n",
      "ORIGINAL: partnerships | LEMMA: partnership | STEM: partnership\n",
      "ORIGINAL: with | LEMMA: with | STEM: with\n",
      "ORIGINAL: industry | LEMMA: industry | STEM: industri\n",
      "ORIGINAL: government | LEMMA: government | STEM: govern\n",
      "ORIGINAL: institutions | LEMMA: institution | STEM: institut\n",
      "ORIGINAL: and | LEMMA: and | STEM: and\n",
      "ORIGINAL: associations | LEMMA: association | STEM: associ\n",
      "ORIGINAL: to | LEMMA: to | STEM: to\n",
      "ORIGINAL: develop | LEMMA: develop | STEM: develop\n",
      "ORIGINAL: high | LEMMA: high | STEM: high\n",
      "ORIGINAL: quality | LEMMA: quality | STEM: qualiti\n",
      "ORIGINAL: programs | LEMMA: program | STEM: program\n",
      "ORIGINAL: that | LEMMA: that | STEM: that\n",
      "ORIGINAL: build | LEMMA: build | STEM: build\n",
      "ORIGINAL: careers | LEMMA: career | STEM: career\n",
      "ORIGINAL: and | LEMMA: and | STEM: and\n",
      "ORIGINAL: help | LEMMA: help | STEM: help\n",
      "ORIGINAL: make | LEMMA: make | STEM: make\n",
      "ORIGINAL: our | LEMMA: our | STEM: our\n",
      "ORIGINAL: communities | LEMMA: community | STEM: commun\n",
      "ORIGINAL: healthier | LEMMA: healthier | STEM: healthier\n",
      "ORIGINAL: and | LEMMA: and | STEM: and\n",
      "ORIGINAL: more | LEMMA: more | STEM: more\n",
      "ORIGINAL: livable | LEMMA: livable | STEM: livabl\n"
     ]
    }
   ],
   "source": [
    "for word in s.words:\n",
    "    print(\"ORIGINAL:\", word, \"| LEMMA:\", word.lemmatize(), \"| STEM:\", word.stem())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example `lemmatize()` mostly just removes pluralization. `stem()` goes further and produces many non-words like \"develop\". It's just a set of rules that try to parse out English morphology. The rule set here is called [Porter stemming](https://snowballstem.org/algorithms/porter/stemmer.html), and there are similar algorithms for [many languages](https://snowballstem.org/algorithms/). Lemmatization is actually smarter because it uses a dictionary, so it can undo common verb inflections... if you tell TextBlob that the word is a verb (to be fair, it can [figure out parts of speech](https://textblob.readthedocs.io/en/dev/quickstart.html#part-of-speech-tagging) if you ask it to.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"running\").words[0].lemmatize('v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"ran\").words[0].lemmatize('v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our purposes we're going to use TexBlob to make a basic tokenizing function that just makes everything lowercase and throws token with less than 3 characters, which throws out punctuation tokens too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(s):\n",
    "    blob = TextBlob(s.lower())\n",
    "    words = [token for token in blob.words if len(token)>2]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document vectors\n",
    "\n",
    "We're going to develop ways of turning a document into a vector in a high dimensional space -- that is, a list of hundreds or throusands of numbers. \n",
    "\n",
    "Why? Well, we're going to make each of the numbers correspond to the score or importance of a word. This is sort of an abstract word cloud, and it can help us to summarize documents.\n",
    "\n",
    "We're also going to use vectors to compare documents to each other. This is useful for many things:\n",
    "- matching search queries against documents\n",
    "- classifying documents (think of this as sorting them into piles)\n",
    "- clustering documents by topic\n",
    "\n",
    "To turn documents into vectors, we need to write a function that takes a string and returns a list of numbers. Our first attempt at this will be by counting the number of tokens of each kind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doc2vec_count(s):\n",
    "    tokens = tokenize(s)\n",
    "    vec = {}\n",
    "    for t in tokens:\n",
    "        vec[t] = vec.get(t, 0) + 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 1, 'cat': 1, 'mat': 1, 'the': 2}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec_count(\"the cat and the mat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use document vectors to summarize documents by imagining them as as list of top words. Let's sort document vectors by decreasing value to try to get an idea of what the entire press release document is \"about\", and print the top 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_sorted_vector(v):\n",
    "    # this \"lambda\" thing is an anonymous function, google me to unluck bonus coding knowledge\n",
    "    sorted_list = sorted(v.items(), key=lambda x: (x[1],x[0]), reverse=True) \n",
    "    sorted_list = sorted_list[:20]\n",
    "    print('\\n'.join([str(x) for x in sorted_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('and', 23)\n",
      "('the', 13)\n",
      "('transportation', 7)\n",
      "('transit', 7)\n",
      "('nti', 7)\n",
      "('with', 5)\n",
      "('training', 5)\n",
      "('that', 5)\n",
      "('public', 5)\n",
      "('this', 4)\n",
      "('our', 4)\n",
      "('for', 4)\n",
      "('workers', 3)\n",
      "('work', 3)\n",
      "('will', 3)\n",
      "('said', 3)\n",
      "('rutgers', 3)\n",
      "('menendez', 3)\n",
      "('lautenberg', 3)\n",
      "('institute', 3)\n"
     ]
    }
   ],
   "source": [
    "print_sorted_vector(doc2vec_count(press_release_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad... but is this press release really \"about\" the words like \"the\" and \"and\"? We're going to need something better. That something for us is TF-IDF term weighting, and we'll talk about it below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing document vectors\n",
    "The simplest way to compare two word count vectors is to count the number of overlapping words. Each word can appear more than once, so we'll multiply together the counts of the same word in each docmument. Why? Because this leads us to a Euclidian feature vector with natural geometric properties, which makes it possible to think about wbat is going on with spatial analogies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doc_similarity(a_vec,b_vec):\n",
    "    total = 0\n",
    "    for word in a_vec:\n",
    "        if word in b_vec:\n",
    "            total += a_vec[word]*b_vec[word]\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = doc2vec_count(str(doc.sentences[6]))  # need str to convert Sentence object to string\n",
    "b = doc2vec_count(str(doc.sentences[9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nti': 1, 'work': 1, 'essential': 1, 'for': 1, 'the': 1, 'industry': 1, 'continue': 1, 'create': 1, 'good': 1, 'long': 1, 'term': 1, 'jobs': 2, 'provide': 1, 'families': 1, 'with': 1, 'access': 1, 'opportunity': 1, 'and': 6, 'help': 1, 'our': 1, 'communities': 1, 'grow': 1, 'ways': 1, 'that': 1, 'are': 1, 'smart': 1, 'efficient': 2, 'millions': 1, 'people': 1, 'count': 1, 'reliable': 1, 'public': 1, 'transportation': 1, 'get': 1, 'from': 1, 'their': 2, 'homes': 1, 'transit': 1, 'workers': 1, 'make': 1, 'this': 1, 'possible': 1, 'said': 1, 'senator': 1, 'lautenberg': 1}\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 1, 'grant': 1, 'will': 1, 'ensure': 1, 'that': 3, 'the': 3, 'existing': 1, 'and': 6, 'importantly': 1, 'new': 1, 'generation': 1, 'public': 1, 'transportation': 1, 'workers': 1, 'receive': 1, 'training': 1, 'they': 1, 'need': 1, 'comply': 1, 'with': 2, 'federal': 1, 'regulations': 1, 'operate': 1, 'safe': 1, 'efficient': 1, 'transit': 1, 'services': 1, 'said': 1, 'nti': 1, 'director': 1, 'paul': 1, 'larrousse.nti': 1, 'works': 1, 'cooperatively': 1, 'through': 1, 'partnerships': 1, 'industry': 1, 'government': 1, 'institutions': 1, 'associations': 1, 'develop': 1, 'high': 1, 'quality': 1, 'programs': 1, 'build': 1, 'careers': 1, 'help': 1, 'make': 1, 'our': 1, 'communities': 1, 'healthier': 1, 'more': 1, 'livable': 1}\n"
     ]
    }
   ],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_similarity(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, but what does \"58\" mean? One problem we are going to have is that longer documents will tend to be more similar to everything else. More words mean more words can match. We will solve this problem by normalizing each document vector so that it has length 1, meaning that the sum of the _squares_ of the elements is one -- this is Pyhagoras, so we can think of a document as a unit vector now, or a direction, in a space that has as many dimensions as the vocabulary size. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doc2vec_normalized(s):\n",
    "    tokens = tokenize(s)\n",
    "    vec = {}\n",
    "    for t in tokens:\n",
    "        vec[t] = vec.get(t, 0) + 1 # get from dict with a default of 0 if missing\n",
    "        \n",
    "    length = math.sqrt(sum([x*x for x in vec.values()]))  # length of a vector, according to Pythagoras\n",
    "    for word,value in vec.items():\n",
    "        vec[word] /= length\n",
    "        \n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = doc2vec_normalized(str(doc.sentences[6]))  # need str to convert Sentence object to string\n",
    "b = doc2vec_normalized(str(doc.sentences[9]))\n",
    "c = doc2vec_normalized(str(doc.sentences[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'today': 0.1690308509457033, 'with': 0.3380617018914066, 'gas': 0.1690308509457033, 'prices': 0.1690308509457033, 'around': 0.1690308509457033, 'gallon': 0.1690308509457033, 'and': 0.3380617018914066, 'oil': 0.1690308509457033, 'companies': 0.1690308509457033, 'reaping': 0.1690308509457033, 'record': 0.1690308509457033, 'profits': 0.1690308509457033, 'the': 0.3380617018914066, 'threat': 0.1690308509457033, 'climate': 0.1690308509457033, 'change': 0.1690308509457033, 'growing': 0.1690308509457033, 'wealth': 0.1690308509457033, 'disparity': 0.1690308509457033, 'transit': 0.1690308509457033, 'part': 0.1690308509457033, 'solution': 0.1690308509457033, 'for': 0.1690308509457033, 'number': 0.1690308509457033, 'interconnected': 0.1690308509457033, 'challenges': 0.1690308509457033}\n"
     ]
    }
   ],
   "source": [
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our similarity function says that the first two sentences are the most similar, because they have words like \"industry\", \"efficient\", and \"transit\" in common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5943484047696529\n",
      "0.37583907018239515\n",
      "0.32251021858460976\n"
     ]
    }
   ],
   "source": [
    "print(doc_similarity(a,b))\n",
    "print(doc_similarity(b,c))\n",
    "print(doc_similarity(a,c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF weighting\n",
    "As we discussed in class, term frequency / inverse document frequency is a word weighting scheme that tries to give less weight to words that appear in many documents. This will solve our \"the\" problem, and it will also help drop out topic words that are common to the entire corpus. Rather than writing it ourselves, we're going to use the implementation in the `scikit` library.\n",
    "\n",
    "Scikit includes a bunch of built in vectorizers, such as classic counting. Let's turn the first ten press releases into vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1,105,000</th>\n",
       "      <th>1,160,000</th>\n",
       "      <th>100</th>\n",
       "      <th>1099</th>\n",
       "      <th>13,381</th>\n",
       "      <th>142nd</th>\n",
       "      <th>15th</th>\n",
       "      <th>170</th>\n",
       "      <th>170,000</th>\n",
       "      <th>179,550</th>\n",
       "      <th>...</th>\n",
       "      <th>yanukovych</th>\n",
       "      <th>year</th>\n",
       "      <th>yearly</th>\n",
       "      <th>years</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>yulia</th>\n",
       "      <th>–through</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 1261 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1,105,000  1,160,000  100  1099  13,381  142nd  15th  170  170,000  \\\n",
       "0          0          0    2     0       0      1     1    0        0   \n",
       "1          0          0    0     0       0      0     0    0        1   \n",
       "2          0          0    1     0       0      0     0    0        0   \n",
       "3          0          0    0     0       0      0     0    0        0   \n",
       "4          1          1    0     0       0      0     0    0        0   \n",
       "5          0          0    0     0       0      0     0    0        0   \n",
       "6          0          0    0     0       1      0     0    0        0   \n",
       "7          0          0    1     1       0      0     0    1        0   \n",
       "8          0          0    0     0       0      0     0    0        0   \n",
       "9          0          0    0     0       0      0     0    0        0   \n",
       "\n",
       "   179,550    ...     yanukovych  year  yearly  years  yesterday  york  young  \\\n",
       "0        0    ...              0     0       0      0          0     0      0   \n",
       "1        0    ...              0     1       0      1          1     0      0   \n",
       "2        0    ...              0     3       1      0          0     0      0   \n",
       "3        0    ...              0     0       0      0          0     0      0   \n",
       "4        0    ...              0     0       0      0          0     1      3   \n",
       "5        0    ...              0     4       0      2          0     0      0   \n",
       "6        1    ...              0     0       0      0          0     0      0   \n",
       "7        0    ...              0     1       0      3          0     0      0   \n",
       "8        0    ...              1     0       0      1          0     0      0   \n",
       "9        0    ...              0     0       0      0          0     0      0   \n",
       "\n",
       "   younger  yulia  –through  \n",
       "0        0      0         0  \n",
       "1        0      0         0  \n",
       "2        0      0         0  \n",
       "3        0      0         0  \n",
       "4        0      0         0  \n",
       "5        0      0         0  \n",
       "6        0      0         0  \n",
       "7        0      0         0  \n",
       "8        1      6         0  \n",
       "9        0      0         1  \n",
       "\n",
       "[10 rows x 1261 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a new Count Vectorizer!!!! It will conveniently remove stop words if we tell it what language we're using\n",
    "vectorizer = CountVectorizer(stop_words='english', tokenizer=tokenize)\n",
    "\n",
    "# Use the vectorizor we just made. The name fit_transform will be clearer later when we use it for machine learning\n",
    "matrix = vectorizer.fit_transform(pr.text[0:10])\n",
    "\n",
    "# The easiest way to see what happenned is to make a dataframe\n",
    "results = pd.DataFrame(matrix.toarray(), columns=vectorizer.get_feature_names())\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row is a document, and each column corresponds to a single vocabulary word or token. And there are a lot of columns, which means we can think of these as points in 1,261 dimensional space.\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,105,000',\n",
       " '1,160,000',\n",
       " '100',\n",
       " '1099',\n",
       " '13,381',\n",
       " '142nd',\n",
       " '15th',\n",
       " '170',\n",
       " '170,000',\n",
       " '179,550',\n",
       " '1903',\n",
       " '1980s',\n",
       " '1983',\n",
       " '1996',\n",
       " '1998',\n",
       " '20,000',\n",
       " '2008',\n",
       " '2009',\n",
       " '2010',\n",
       " '2011',\n",
       " '2012',\n",
       " '265,950',\n",
       " '3.8',\n",
       " '315,000',\n",
       " '335',\n",
       " '350',\n",
       " '351,554',\n",
       " '4,600',\n",
       " '47,200',\n",
       " '5,000',\n",
       " '51.5m',\n",
       " '519',\n",
       " '5:00',\n",
       " '5million',\n",
       " '6,400',\n",
       " '6.5',\n",
       " '650,000',\n",
       " '65m',\n",
       " '7.8',\n",
       " '750,000',\n",
       " '770,000',\n",
       " '771',\n",
       " '929,088',\n",
       " '96-3',\n",
       " 'able',\n",
       " 'abuse',\n",
       " 'access',\n",
       " 'according',\n",
       " 'accountability',\n",
       " 'acquisition',\n",
       " 'act',\n",
       " 'action',\n",
       " 'add',\n",
       " 'added',\n",
       " 'additionally',\n",
       " 'address',\n",
       " 'addresses',\n",
       " 'adopted',\n",
       " 'advance',\n",
       " 'adversely',\n",
       " 'advocate',\n",
       " 'affect',\n",
       " 'affected',\n",
       " 'affiliates',\n",
       " 'afford',\n",
       " 'afg',\n",
       " 'afin',\n",
       " 'african',\n",
       " 'african-american',\n",
       " 'african-americans',\n",
       " 'agencies',\n",
       " 'agency',\n",
       " 'agenda',\n",
       " 'agents',\n",
       " 'ahmadinejad',\n",
       " 'aid',\n",
       " 'air',\n",
       " 'alarm',\n",
       " 'ali',\n",
       " 'allow',\n",
       " 'allows',\n",
       " 'ambassador',\n",
       " 'amendment',\n",
       " 'amendments',\n",
       " 'america',\n",
       " 'american',\n",
       " 'americans',\n",
       " 'anniversary',\n",
       " 'announce',\n",
       " 'announced',\n",
       " 'annual',\n",
       " 'ansar-e-hezbollah',\n",
       " 'apologize',\n",
       " 'apologized',\n",
       " 'applaud',\n",
       " 'applauded',\n",
       " 'applauds',\n",
       " 'applications',\n",
       " 'apply',\n",
       " 'appreciation',\n",
       " 'appropriations',\n",
       " 'approval',\n",
       " 'arabia',\n",
       " 'areas',\n",
       " 'aside',\n",
       " 'asked',\n",
       " 'assembly',\n",
       " 'assess',\n",
       " 'assets',\n",
       " 'assist',\n",
       " 'assistance',\n",
       " 'assistants',\n",
       " 'association',\n",
       " 'atlantic',\n",
       " 'attach',\n",
       " 'attack',\n",
       " 'attempt',\n",
       " 'attended',\n",
       " 'authorities',\n",
       " 'authority',\n",
       " 'authorization',\n",
       " 'available',\n",
       " 'average',\n",
       " 'awarded',\n",
       " 'bank',\n",
       " 'banking',\n",
       " 'banks',\n",
       " 'bank—all',\n",
       " 'baptist',\n",
       " 'barracks',\n",
       " 'bars',\n",
       " 'base',\n",
       " 'based',\n",
       " 'basij-e',\n",
       " 'beginning',\n",
       " 'beliefs',\n",
       " 'benefit',\n",
       " 'benefits',\n",
       " 'bergen/passaic',\n",
       " 'best',\n",
       " 'bigger',\n",
       " 'biggest',\n",
       " 'billion',\n",
       " 'bills',\n",
       " 'bionj',\n",
       " 'biotechnology',\n",
       " 'bipartisan',\n",
       " 'black',\n",
       " 'blueprint',\n",
       " 'bomb',\n",
       " 'bombing',\n",
       " 'bono',\n",
       " 'boon',\n",
       " 'boost',\n",
       " 'box',\n",
       " 'branch',\n",
       " 'brand',\n",
       " 'breast',\n",
       " 'brennan',\n",
       " 'brigadier',\n",
       " 'brinker',\n",
       " 'bro-bono',\n",
       " 'broader',\n",
       " 'brokering',\n",
       " 'brown',\n",
       " 'budget',\n",
       " 'built',\n",
       " 'business',\n",
       " 'businesses',\n",
       " 'buster',\n",
       " 'buy',\n",
       " 'bylaws',\n",
       " 'cabinet',\n",
       " 'called',\n",
       " 'cancer',\n",
       " 'cancers',\n",
       " 'capacity',\n",
       " 'capital',\n",
       " 'capitol',\n",
       " 'capture',\n",
       " 'care',\n",
       " 'career',\n",
       " 'cargo',\n",
       " 'cars',\n",
       " 'case',\n",
       " 'caused',\n",
       " 'cecile',\n",
       " 'celebrate',\n",
       " 'celebrated',\n",
       " 'celebration',\n",
       " 'cells',\n",
       " 'center',\n",
       " 'centers',\n",
       " 'central',\n",
       " 'centralized',\n",
       " 'certain',\n",
       " 'chair',\n",
       " 'chairman',\n",
       " 'championed',\n",
       " 'chance',\n",
       " 'change',\n",
       " 'chapter',\n",
       " 'charge',\n",
       " 'cheaper',\n",
       " 'check-up',\n",
       " 'children',\n",
       " 'chip',\n",
       " 'choice',\n",
       " 'church',\n",
       " 'circumvent',\n",
       " 'cisada',\n",
       " 'citibank',\n",
       " 'citizens',\n",
       " 'city',\n",
       " 'clarifies',\n",
       " 'clean',\n",
       " 'cleaner',\n",
       " 'cleanings',\n",
       " 'clearinghouse',\n",
       " 'climate',\n",
       " 'clinic',\n",
       " 'clinical',\n",
       " 'clinics',\n",
       " 'closer',\n",
       " 'co-authored',\n",
       " 'co-sponsor',\n",
       " 'co-sponsored',\n",
       " 'coalition',\n",
       " 'colleagues',\n",
       " 'columbia',\n",
       " 'comes',\n",
       " 'commission',\n",
       " 'commitment',\n",
       " 'committed',\n",
       " 'committee',\n",
       " 'communications',\n",
       " 'communities',\n",
       " 'community',\n",
       " 'commuters',\n",
       " 'companies',\n",
       " 'company',\n",
       " 'company/tanker',\n",
       " 'compels',\n",
       " 'compensate',\n",
       " 'competitive',\n",
       " 'comprehensive',\n",
       " 'concluded',\n",
       " 'concludes',\n",
       " 'condition',\n",
       " 'conditions',\n",
       " 'conducive',\n",
       " 'congestion',\n",
       " 'congress',\n",
       " 'congressional',\n",
       " 'consider',\n",
       " 'consolidation',\n",
       " 'container',\n",
       " 'containers',\n",
       " 'contains',\n",
       " 'continue',\n",
       " 'contracts',\n",
       " 'contributions',\n",
       " 'convince',\n",
       " 'cooperative',\n",
       " 'coordinate',\n",
       " 'core',\n",
       " 'corps',\n",
       " 'corrupt',\n",
       " 'countless',\n",
       " 'country',\n",
       " 'course',\n",
       " 'court',\n",
       " 'coverage',\n",
       " 'craft',\n",
       " 'create',\n",
       " 'created',\n",
       " 'creates',\n",
       " 'creating',\n",
       " 'creation',\n",
       " 'credit',\n",
       " 'crowns',\n",
       " 'cruelty',\n",
       " 'crunch',\n",
       " 'culmination',\n",
       " 'cure',\n",
       " 'currency',\n",
       " 'customer',\n",
       " 'cut',\n",
       " 'cutting',\n",
       " 'd-nj',\n",
       " 'd.c',\n",
       " 'day',\n",
       " 'days',\n",
       " 'deal',\n",
       " 'dean',\n",
       " 'dear',\n",
       " 'death',\n",
       " 'debbie',\n",
       " 'december',\n",
       " 'decision',\n",
       " 'decisions',\n",
       " 'dedication',\n",
       " 'defense',\n",
       " 'deforest',\n",
       " 'delays',\n",
       " 'deliver',\n",
       " 'delivery',\n",
       " 'demand',\n",
       " 'democratic',\n",
       " 'democrats',\n",
       " 'denied',\n",
       " 'dental',\n",
       " 'dentistry',\n",
       " 'dentists',\n",
       " 'deny',\n",
       " 'department',\n",
       " 'departments',\n",
       " 'depend',\n",
       " 'deserve',\n",
       " 'deserves',\n",
       " 'design',\n",
       " 'designated',\n",
       " 'designed',\n",
       " 'designs',\n",
       " 'desire',\n",
       " 'despite',\n",
       " 'destroyed',\n",
       " 'detention',\n",
       " 'determination',\n",
       " 'determined',\n",
       " 'development',\n",
       " 'devised',\n",
       " 'diabetes',\n",
       " 'different',\n",
       " 'dignity',\n",
       " 'diplomatic',\n",
       " 'direct',\n",
       " 'direction',\n",
       " 'directors',\n",
       " 'dirty',\n",
       " 'disabled',\n",
       " 'disappointment',\n",
       " 'discovery',\n",
       " 'discuss',\n",
       " 'discussion',\n",
       " 'disease',\n",
       " 'diseases',\n",
       " 'district',\n",
       " 'diverse',\n",
       " 'diversity',\n",
       " 'dodd-frank',\n",
       " 'does',\n",
       " 'doing',\n",
       " 'dollar',\n",
       " 'dollars',\n",
       " 'don',\n",
       " 'downturn',\n",
       " 'dream',\n",
       " 'drop',\n",
       " 'drowning',\n",
       " 'earlier',\n",
       " 'earmarks',\n",
       " 'earn',\n",
       " 'earnestly',\n",
       " 'ease',\n",
       " 'easier',\n",
       " 'easing',\n",
       " 'east',\n",
       " 'eat',\n",
       " 'economic',\n",
       " 'economies',\n",
       " 'economy',\n",
       " 'education',\n",
       " 'educational',\n",
       " 'effectively',\n",
       " 'efforts',\n",
       " 'elderly',\n",
       " 'electronically',\n",
       " 'eligible',\n",
       " 'eliminating',\n",
       " 'embargo',\n",
       " 'emergencies',\n",
       " 'emergency',\n",
       " 'employees',\n",
       " 'empowered',\n",
       " 'enactment',\n",
       " 'encouraged',\n",
       " 'encourages',\n",
       " 'end',\n",
       " 'endure',\n",
       " 'energy',\n",
       " 'enforce',\n",
       " 'engaging',\n",
       " 'enhance',\n",
       " 'enormous',\n",
       " 'enriched',\n",
       " 'ensure',\n",
       " 'entire',\n",
       " 'entities',\n",
       " 'environment',\n",
       " 'equipment',\n",
       " 'equipped',\n",
       " 'era',\n",
       " 'especially',\n",
       " 'establishes',\n",
       " 'eugenia',\n",
       " 'european',\n",
       " 'event',\n",
       " 'example',\n",
       " 'exams',\n",
       " 'exchange',\n",
       " 'executive',\n",
       " 'existing',\n",
       " 'expand',\n",
       " 'expands',\n",
       " 'expel',\n",
       " 'expensing',\n",
       " 'expensive',\n",
       " 'experienced',\n",
       " 'experts',\n",
       " 'explicitly',\n",
       " 'explored',\n",
       " 'exposed',\n",
       " 'express',\n",
       " 'expressed',\n",
       " 'expressing',\n",
       " 'extractions',\n",
       " 'extraordinary',\n",
       " 'fabric',\n",
       " 'face',\n",
       " 'facilitate',\n",
       " 'facilitating',\n",
       " 'facing',\n",
       " 'factory',\n",
       " 'faith',\n",
       " 'families',\n",
       " 'family',\n",
       " 'far',\n",
       " 'fault',\n",
       " 'favor',\n",
       " 'february',\n",
       " 'federal',\n",
       " 'feldman',\n",
       " 'fema',\n",
       " 'fight',\n",
       " 'fighters',\n",
       " 'fighting',\n",
       " 'fillings',\n",
       " 'finally',\n",
       " 'financial',\n",
       " 'firefighters',\n",
       " 'fix',\n",
       " 'fleet',\n",
       " 'flourish',\n",
       " 'flow',\n",
       " 'fluoride',\n",
       " 'focus',\n",
       " 'following',\n",
       " 'foods',\n",
       " 'foreign',\n",
       " 'forth',\n",
       " 'forum',\n",
       " 'forward',\n",
       " 'fought',\n",
       " 'foundation',\n",
       " 'frank',\n",
       " 'free',\n",
       " 'freed',\n",
       " 'freedom',\n",
       " 'friday',\n",
       " 'fuel',\n",
       " 'fuels',\n",
       " 'fully',\n",
       " 'fun',\n",
       " 'fund',\n",
       " 'fundamental',\n",
       " 'funding',\n",
       " 'funds',\n",
       " 'future',\n",
       " 'gain',\n",
       " 'gambled',\n",
       " 'gardens',\n",
       " 'gas',\n",
       " 'general',\n",
       " 'generated',\n",
       " 'generating',\n",
       " 'getting',\n",
       " 'giants',\n",
       " 'gillibrand',\n",
       " 'given',\n",
       " 'gives',\n",
       " 'giving',\n",
       " 'goals',\n",
       " 'goes',\n",
       " 'good',\n",
       " 'goodwill',\n",
       " 'government',\n",
       " 'governments',\n",
       " 'grant',\n",
       " 'grants',\n",
       " 'great',\n",
       " 'greater',\n",
       " 'greatly',\n",
       " 'greatness',\n",
       " 'grow',\n",
       " 'grown',\n",
       " 'growth',\n",
       " 'guaranteed',\n",
       " 'guarantees',\n",
       " 'guard',\n",
       " 'guilty',\n",
       " 'hailed',\n",
       " 'hails',\n",
       " 'half',\n",
       " 'halt',\n",
       " 'hamp',\n",
       " 'handle',\n",
       " 'hard',\n",
       " 'hardship',\n",
       " 'hart',\n",
       " 'health',\n",
       " 'healthier',\n",
       " 'healthy',\n",
       " 'hear',\n",
       " 'heard',\n",
       " 'hearing',\n",
       " 'held',\n",
       " 'help',\n",
       " 'helped',\n",
       " 'helping',\n",
       " 'helps',\n",
       " 'hemisphere',\n",
       " 'highlands',\n",
       " 'hill',\n",
       " 'hinders',\n",
       " 'hiring',\n",
       " 'historically',\n",
       " 'history',\n",
       " 'homeland',\n",
       " 'homeowners',\n",
       " 'homes',\n",
       " 'hometown',\n",
       " 'honor',\n",
       " 'honored',\n",
       " 'honors',\n",
       " 'hope',\n",
       " 'hopes',\n",
       " 'host',\n",
       " 'hosted',\n",
       " 'hosts',\n",
       " 'house',\n",
       " 'housing',\n",
       " 'http',\n",
       " 'hubs',\n",
       " 'human',\n",
       " 'hurt',\n",
       " 'hygiene',\n",
       " 'hygienists',\n",
       " 'illicit',\n",
       " 'immigration',\n",
       " 'impact',\n",
       " 'importance',\n",
       " 'important',\n",
       " 'impose',\n",
       " 'imposed',\n",
       " 'imposes',\n",
       " 'imposition',\n",
       " 'impression',\n",
       " 'imprisonment',\n",
       " 'improve',\n",
       " 'improvements',\n",
       " 'incentives',\n",
       " 'include',\n",
       " 'included',\n",
       " 'including',\n",
       " 'inclusion',\n",
       " 'income',\n",
       " 'inconsistent',\n",
       " 'increase',\n",
       " 'increased',\n",
       " 'increasing',\n",
       " 'incredible',\n",
       " 'incredibly',\n",
       " 'industry',\n",
       " 'inform',\n",
       " 'information',\n",
       " 'infrastructure',\n",
       " 'inhumane',\n",
       " 'initiatives',\n",
       " 'insider',\n",
       " 'inspired',\n",
       " 'instance',\n",
       " 'instances',\n",
       " 'institute',\n",
       " 'institutions',\n",
       " 'instruction',\n",
       " 'insurance',\n",
       " 'integrity',\n",
       " 'intelligence',\n",
       " 'interbank',\n",
       " 'interests',\n",
       " 'international',\n",
       " 'internet',\n",
       " 'introduced',\n",
       " 'invest',\n",
       " 'investigate',\n",
       " 'investment',\n",
       " 'investments',\n",
       " 'investors',\n",
       " 'iran',\n",
       " 'iranian',\n",
       " 'iranian-energy',\n",
       " 'irgc',\n",
       " 'irvington',\n",
       " 'islamic',\n",
       " 'iso',\n",
       " 'issue',\n",
       " 'issued',\n",
       " 'jack',\n",
       " 'jail',\n",
       " 'jeffrey',\n",
       " 'jersey',\n",
       " 'jim',\n",
       " 'job',\n",
       " 'jobs',\n",
       " 'joined',\n",
       " 'joint',\n",
       " 'judgment',\n",
       " 'judy',\n",
       " 'just',\n",
       " 'justice',\n",
       " 'khameini',\n",
       " 'khobar',\n",
       " 'kids',\n",
       " 'killed',\n",
       " 'kirk',\n",
       " 'knowledge',\n",
       " 'komen',\n",
       " 'korea',\n",
       " 'lack',\n",
       " 'laid',\n",
       " 'landing',\n",
       " 'language',\n",
       " 'largest',\n",
       " 'later',\n",
       " 'latinas',\n",
       " 'laureldale',\n",
       " 'lautenberg',\n",
       " 'law',\n",
       " 'laws',\n",
       " 'lay',\n",
       " 'leader',\n",
       " 'leaders',\n",
       " 'learn',\n",
       " 'leases',\n",
       " 'lebanon',\n",
       " 'led',\n",
       " 'legislation',\n",
       " 'legislators',\n",
       " 'let',\n",
       " 'letter',\n",
       " 'liability',\n",
       " 'life',\n",
       " 'life-saving',\n",
       " 'lifeline',\n",
       " 'like',\n",
       " 'limits',\n",
       " 'lincoln',\n",
       " 'line',\n",
       " 'lines',\n",
       " 'literally',\n",
       " 'livable',\n",
       " 'lives',\n",
       " 'living',\n",
       " 'loan',\n",
       " 'local',\n",
       " 'logical',\n",
       " 'long',\n",
       " 'lost',\n",
       " 'low',\n",
       " 'low-income',\n",
       " 'lucky',\n",
       " 'machine',\n",
       " 'maintain',\n",
       " 'major',\n",
       " 'make',\n",
       " 'makes',\n",
       " 'making',\n",
       " 'mammograms',\n",
       " 'management',\n",
       " 'mandatory',\n",
       " 'marine',\n",
       " 'mark',\n",
       " 'market',\n",
       " 'materials',\n",
       " 'mayor',\n",
       " 'mays',\n",
       " 'mean',\n",
       " 'mean-spirited',\n",
       " 'means',\n",
       " 'measures',\n",
       " 'medicaid',\n",
       " 'medical',\n",
       " 'medically',\n",
       " 'medicare',\n",
       " 'medicine',\n",
       " 'medium',\n",
       " 'meet',\n",
       " 'meeting',\n",
       " 'mellat',\n",
       " 'members',\n",
       " 'men',\n",
       " 'menendez',\n",
       " 'message',\n",
       " 'messages',\n",
       " 'met',\n",
       " 'military',\n",
       " 'million',\n",
       " 'millions',\n",
       " 'minimal',\n",
       " 'minister',\n",
       " 'ministry',\n",
       " 'mission',\n",
       " 'mobile',\n",
       " 'modern',\n",
       " 'modification',\n",
       " 'money',\n",
       " 'month',\n",
       " 'moorestown',\n",
       " 'morning',\n",
       " 'mortgage',\n",
       " 'motaz',\n",
       " 'mother',\n",
       " 'motivated',\n",
       " 'multi-million',\n",
       " 'municipalities',\n",
       " 'nation',\n",
       " 'national',\n",
       " 'nationally',\n",
       " 'nearly',\n",
       " 'necessary',\n",
       " 'need',\n",
       " 'needed',\n",
       " 'neighborhood',\n",
       " 'new',\n",
       " 'newark',\n",
       " 'news',\n",
       " 'nioc',\n",
       " 'nitc',\n",
       " 'nonprofit',\n",
       " 'nonpublic',\n",
       " 'noose',\n",
       " 'north',\n",
       " 'northern',\n",
       " 'notes',\n",
       " 'nti',\n",
       " 'nuclear',\n",
       " 'number',\n",
       " 'nutley',\n",
       " 'obama',\n",
       " 'october',\n",
       " 'offered',\n",
       " 'office',\n",
       " 'officials',\n",
       " 'oil',\n",
       " 'older',\n",
       " 'ongoing',\n",
       " 'operating',\n",
       " 'operations',\n",
       " 'opportunities',\n",
       " 'opportunity',\n",
       " 'opposition',\n",
       " 'oppression',\n",
       " 'oral',\n",
       " 'order',\n",
       " 'organizations',\n",
       " 'oriented',\n",
       " 'outside',\n",
       " 'outstanding',\n",
       " 'overall',\n",
       " 'overwhelming',\n",
       " 'owner',\n",
       " 'owners',\n",
       " 'owners–',\n",
       " 'owning',\n",
       " 'p.m',\n",
       " 'pap',\n",
       " 'paperwork',\n",
       " 'paramilitary',\n",
       " 'parenthood',\n",
       " 'parents',\n",
       " 'participated',\n",
       " 'particularly',\n",
       " 'partisan',\n",
       " 'partnered',\n",
       " 'party',\n",
       " 'passage',\n",
       " 'passed',\n",
       " 'past',\n",
       " 'pastor',\n",
       " 'path',\n",
       " 'patients',\n",
       " 'pay',\n",
       " 'paying',\n",
       " 'payment',\n",
       " 'payroll',\n",
       " 'people',\n",
       " 'percent',\n",
       " 'performed',\n",
       " 'personal',\n",
       " 'physical',\n",
       " 'pioneering',\n",
       " 'pioneers',\n",
       " 'place',\n",
       " 'plan',\n",
       " 'planned',\n",
       " 'planning',\n",
       " 'pleased',\n",
       " 'policy',\n",
       " 'political',\n",
       " 'politically',\n",
       " 'politics',\n",
       " 'possible',\n",
       " 'post',\n",
       " 'potential',\n",
       " 'potentially',\n",
       " 'power',\n",
       " 'praised',\n",
       " 'praises',\n",
       " 'present',\n",
       " 'president',\n",
       " 'pressure',\n",
       " 'preventative',\n",
       " 'prevented',\n",
       " 'prevention',\n",
       " 'prices',\n",
       " 'primary',\n",
       " 'prime',\n",
       " 'principal',\n",
       " 'principles',\n",
       " 'prison',\n",
       " 'private',\n",
       " 'pro',\n",
       " 'pro-bono',\n",
       " 'problems',\n",
       " 'procedure',\n",
       " 'procedures',\n",
       " 'process',\n",
       " 'produced',\n",
       " 'products',\n",
       " 'profit',\n",
       " 'program',\n",
       " 'programs',\n",
       " 'prohibition',\n",
       " 'project',\n",
       " 'projects',\n",
       " 'promote',\n",
       " 'proposal',\n",
       " 'prosecute',\n",
       " 'prosecuting',\n",
       " 'prosperity',\n",
       " 'protect',\n",
       " 'proud',\n",
       " 'provide',\n",
       " 'provided',\n",
       " 'provider',\n",
       " 'providers',\n",
       " 'provides',\n",
       " 'providing',\n",
       " 'provision',\n",
       " 'provisions',\n",
       " 'public',\n",
       " 'purchase',\n",
       " 'putting',\n",
       " 'qualify',\n",
       " 'qualifying',\n",
       " 'quality',\n",
       " 'quickly',\n",
       " 'r-il',\n",
       " 'r-mi',\n",
       " 'race',\n",
       " 'raise',\n",
       " 'rank',\n",
       " 'rates',\n",
       " 'ratification',\n",
       " 'reach',\n",
       " 'read',\n",
       " 'ready',\n",
       " 'real',\n",
       " 'reason',\n",
       " 'reauthorization',\n",
       " 'rebuild',\n",
       " 'receive',\n",
       " 'received',\n",
       " 'receiving',\n",
       " 'recent',\n",
       " 'recently',\n",
       " 'recognize',\n",
       " 'recommended',\n",
       " 'reconsider',\n",
       " 'reduce',\n",
       " 'referrals',\n",
       " 'refinancing',\n",
       " 'refocus',\n",
       " 'reformed',\n",
       " 'reforms',\n",
       " 'refrigerated',\n",
       " 'refueling',\n",
       " 'regime',\n",
       " 'regimes',\n",
       " 'regulations',\n",
       " 'regulatory',\n",
       " 'rehabilitation',\n",
       " 'reinvigorate',\n",
       " 'rejected',\n",
       " 'relations',\n",
       " 'released',\n",
       " 'releasing',\n",
       " 'relief',\n",
       " 'relieve',\n",
       " 'relieved',\n",
       " 'religion',\n",
       " 'remains',\n",
       " 'remind',\n",
       " 'reminder',\n",
       " 'renal',\n",
       " 'rendered',\n",
       " 'repair',\n",
       " 'repeal',\n",
       " 'repeating',\n",
       " 'replicated',\n",
       " 'report',\n",
       " 'reporting',\n",
       " 'representative',\n",
       " 'reputation',\n",
       " 'requirement',\n",
       " 'requires',\n",
       " 'rescue',\n",
       " 'reserves',\n",
       " 'resources',\n",
       " 'respect',\n",
       " 'respecting',\n",
       " 'respond',\n",
       " 'responders',\n",
       " 'response',\n",
       " 'responsible',\n",
       " 'restore',\n",
       " 'restoring',\n",
       " 'restrictions',\n",
       " 'resulted',\n",
       " 'results',\n",
       " 'retirements',\n",
       " 'rev',\n",
       " 'revenue',\n",
       " 'reversed',\n",
       " 'reversing',\n",
       " 'revolutionary',\n",
       " 'rich',\n",
       " 'ridgewood',\n",
       " 'right',\n",
       " 'rights',\n",
       " 'risking',\n",
       " 'riverton',\n",
       " 'robert',\n",
       " 'roger',\n",
       " 'roll',\n",
       " 'rooted',\n",
       " 'roughly',\n",
       " 'roundtable',\n",
       " 'russia',\n",
       " 'rutgers',\n",
       " 's.1048',\n",
       " 'saderat',\n",
       " 'safe',\n",
       " 'safely',\n",
       " 'safer',\n",
       " 'safety',\n",
       " 'said',\n",
       " 'sake',\n",
       " 'sanction',\n",
       " 'sanctions',\n",
       " 'saudi',\n",
       " 'save',\n",
       " 'saved',\n",
       " 'saving',\n",
       " 'says',\n",
       " 'sba',\n",
       " 'school',\n",
       " 'science',\n",
       " 'scores',\n",
       " 'screening',\n",
       " 'screenings',\n",
       " 'sea',\n",
       " 'second',\n",
       " 'secretary',\n",
       " 'sector',\n",
       " 'sectors',\n",
       " 'secure',\n",
       " 'securing',\n",
       " 'securities',\n",
       " ...]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at all those columns with number names! We didn't remove them in the tokenizer, so they're tokens. Do we want that? It depends! Sometimes the numbers in the text can be interesting information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try this again with tf-idf weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>'01</th>\n",
       "      <th>'activist</th>\n",
       "      <th>'acts</th>\n",
       "      <th>'disappointing</th>\n",
       "      <th>'e-verify</th>\n",
       "      <th>'em</th>\n",
       "      <th>'liberty</th>\n",
       "      <th>'ll</th>\n",
       "      <th>'no</th>\n",
       "      <th>'re</th>\n",
       "      <th>...</th>\n",
       "      <th>…oil-drilling</th>\n",
       "      <th>…struggling</th>\n",
       "      <th>…tarp</th>\n",
       "      <th>…that</th>\n",
       "      <th>…the</th>\n",
       "      <th>…then</th>\n",
       "      <th>…there</th>\n",
       "      <th>…these</th>\n",
       "      <th>…this</th>\n",
       "      <th>…we</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27660 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   '01  'activist  'acts  'disappointing  'e-verify  'em  'liberty  'll  'no  \\\n",
       "0  0.0        0.0    0.0             0.0        0.0  0.0       0.0  0.0  0.0   \n",
       "1  0.0        0.0    0.0             0.0        0.0  0.0       0.0  0.0  0.0   \n",
       "2  0.0        0.0    0.0             0.0        0.0  0.0       0.0  0.0  0.0   \n",
       "3  0.0        0.0    0.0             0.0        0.0  0.0       0.0  0.0  0.0   \n",
       "4  0.0        0.0    0.0             0.0        0.0  0.0       0.0  0.0  0.0   \n",
       "\n",
       "   're ...   …oil-drilling  …struggling  …tarp  …that  …the  …then  …there  \\\n",
       "0  0.0 ...             0.0          0.0    0.0    0.0   0.0    0.0     0.0   \n",
       "1  0.0 ...             0.0          0.0    0.0    0.0   0.0    0.0     0.0   \n",
       "2  0.0 ...             0.0          0.0    0.0    0.0   0.0    0.0     0.0   \n",
       "3  0.0 ...             0.0          0.0    0.0    0.0   0.0    0.0     0.0   \n",
       "4  0.0 ...             0.0          0.0    0.0    0.0   0.0    0.0     0.0   \n",
       "\n",
       "   …these  …this  …we  \n",
       "0     0.0    0.0  0.0  \n",
       "1     0.0    0.0  0.0  \n",
       "2     0.0    0.0  0.0  \n",
       "3     0.0    0.0  0.0  \n",
       "4     0.0    0.0  0.0  \n",
       "\n",
       "[5 rows x 27660 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', tokenizer=tokenize)\n",
    "\n",
    "matrix = vectorizer.fit_transform(pr.text)\n",
    "\n",
    "# The easiest way to see what happenned is to make a dataframe\n",
    "tfidf = pd.DataFrame(matrix.toarray(), columns=vectorizer.get_feature_names())\n",
    "tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the top words in the press release again. The stop words have gone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('nti', 0.56487288350283282)\n",
      "('transit', 0.30992020013592608)\n",
      "('transportation', 0.23490806362768965)\n",
      "('training', 0.20020090792648149)\n",
      "('rutgers', 0.16379220882186274)\n",
      "('4.3', 0.15701624958670632)\n",
      "('institute', 0.15030236459784593)\n",
      "('public', 0.13468667783892418)\n",
      "('efficient', 0.13247842020300446)\n",
      "('workers', 0.11513111737966622)\n",
      "('grant', 0.099558443055257573)\n",
      "('award', 0.098396096503811856)\n",
      "('critical', 0.095532368614887939)\n",
      "('larrousse.nti', 0.091692332362515894)\n",
      "('premiere', 0.086826433625999941)\n",
      "('industry', 0.083008913035465287)\n",
      "('lautenberg', 0.081471107009427737)\n",
      "('interconnected', 0.080696126214690411)\n",
      "('element', 0.080696126214690411)\n",
      "('1991', 0.080696126214690411)\n"
     ]
    }
   ],
   "source": [
    "print_sorted_vector(tfidf.iloc[212])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comapre this to the headline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence(\"Menendez and Lautenberg Applaud USDOT’s $4.3 Million Award to National Transit Institute at Rutgers\n",
       "                    \n",
       "                            Funding will provide job training and education for public transportation workers\n",
       "                    \n",
       "                      August 3, 2011\n",
       "                     WASHINGTON, D.C. – Today, U.S.\")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding similar documents\n",
    "Finally, let's use tf-idf to summarize a group of documents. We'll do this by simply summing together the vectors for each document. Dividing by the number of documents will give us an \"average\" vector, but just a simple sum will point in the same direction (the ratios of the components will be the same.) You can think of this as choosing a direction in the middle of all these documents. \n",
    "\n",
    "So what do the first 100 documents discuss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('new', 4.6380084052119663)\n",
      "('jersey', 4.3405154266179391)\n",
      "('menendez', 3.3653048234731422)\n",
      "('iran', 3.3369335810395864)\n",
      "('program', 2.8096238918970617)\n",
      "('tax', 2.7953701626577092)\n",
      "('2012', 2.7558566301624596)\n",
      "('funding', 2.7341462492374515)\n",
      "('million', 2.5937783756758925)\n",
      "('veterans', 2.5615692819926918)\n",
      "('senator', 2.4405760771314737)\n",
      "('safety', 2.3674326591909973)\n",
      "('year', 2.284279574844704)\n",
      "('2011', 2.2828391557370171)\n",
      "('federal', 2.2330684973455455)\n",
      "('lautenberg', 2.1523007201349902)\n",
      "('senate', 2.1362927146719057)\n",
      "('families', 2.098229232266049)\n",
      "('communities', 1.9405729234847227)\n",
      "('iranian', 1.864004829716446)\n"
     ]
    }
   ],
   "source": [
    "docs = tfidf.iloc[:100,:]\n",
    "total = docs.sum(axis=0)\n",
    "print_sorted_vector(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way we can use these document vectors is to compute the similarity between them.  Conveniently, the TfidfVectorizer already outputs normalized vectors so it's easy to calculate similarity just by taking a dot product.  Two identical documents have similarity 1 and two documents with no words in common have similarity 0. We will reverse this to be \"distance\" so that identical documents have distance 0, and we can look for document pairs which are \"closest\" in the vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doc_distance(a_vec,b_vec):\n",
    "    # First we have to compute similarity. The idea is the same as doc_similarity, but\n",
    "    # because we are using arrays and not dictionaries, we can just multiply all the elements \n",
    "    # together and add the sum. This is what numpy's dot function does\n",
    "    similarity = a_vec.dot(b_vec)\n",
    "\n",
    "    # Because the vectors are already normalized, similarity will be 1 if equal, 0 if disjoint\n",
    "    # We want things the other way around\n",
    "    return 1-similarity\n",
    "\n",
    "# helpful little function for distance between documents i and j\n",
    "def dij(i,j):\n",
    "    return doc_distance(tfidf.iloc[i], tfidf.iloc[j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85775589439211042"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dij(100,200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One we thing we can do with this is find the documents that are most simmilar to any particular document. This is an example-driven search engine. Actually what we'll do here is find the distance to every other document, then sort. We'll use the same document as above for the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(212, -4.4408920985006262e-16),\n",
       " (637, 0.43711053078542184),\n",
       " (5, 0.57345588413445436),\n",
       " (11, 0.58115728608200345),\n",
       " (282, 0.68107726734169893)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_doc = 212\n",
    "\n",
    "# create a list of (document index, distance to that document) tuples \n",
    "closest = [(i, dij(query_doc, i)) for i in range(len(tfidf))]\n",
    "\n",
    "# sort by the second element of the tuple, that is, the distance\n",
    "closest.sort(key=lambda x: x[1])\n",
    "\n",
    "# top 5 closest?\n",
    "closest[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, document 212 is closest to itself (that first distance should be a 0, but floating point dirt prevents it). Let's take a look at the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENATORS MENENDEZ AND LAUTENBERG ANNOUNCE $4 MILLION FOR RUTGERS’ NATIONAL TRANSIT INSTITUTE\n",
      "                    \n",
      "                      June 3, 2010\n",
      "                     WASHINGTON – Today, U.S. Senators Robert Menendez (D-NJ) and Frank R. Lautenberg (D-NJ) announced that the Department of Transportation has awarded $4,300,000 in funds to the National Transit Institute (NTI). The Institute was established in 1991 under the Intermodal Surface Transportation Efficiency Act (ISTEA) with the objective of developing, promoting, and delivering quality training, education, and clearinghouse services for the public transit industry. Its primary objective is to develop, deliver, and promote quality programs and materials via cooperative partnerships with industry, government, institutions, and associations around the country. “As we know well in New Jersey, public transit helps families save time and money and helps clear the air we breathe,” said Menendez. “An emphasis on public transit will be critical to rebuilding our economy because of its potential to create new jobs, cut energy costs and traffic congestion and reduce pollution. This federal investment will not only help support a renewed focus on public transit, but it will help New Jersey and Rutgers continue to lead the way.” “As more people across the country are taking mass transit than ever before, it’s critical that we have the most up to date research to keep our transit systems moving safely and efficiently,” Lautenberg said. “This federal funding will continue the vital research that the National Transit Institute has been conducting since 1991.”  Since its founding in 1991, NTI has developed 149 courses and other training materials in collaboration with the Federal Transit Administration (FTA) and the public transit industry in response to their needs. New courses are developed every year to adapt to changes in the industry. In recent years, NTI modernized its educational approach to include the use of visual digital resources outside the classroom. Including the current fiscal year, NTI has been awarded a total of $79.7 million in federal funds since it was established in 1991. For more information about the National Transit Institute, click here: http://www.ntionline.com/index.asp                                                               ###\n"
     ]
    }
   ],
   "source": [
    "print(pr.text[closest[1][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transportation Subcommittee Chair Says Bipartisan Senate Transit Bill will  Deliver $63 Million More Per Year to NJ\n",
      "                    \n",
      "                            More funding means more jobs, easier travel\n",
      "                    \n",
      "                      February 2, 2012\n",
      "                     WASHINGTON – Following today’s approval by the Senate Banking Committee of the Federal Public Transportation Act of 2012, U.S. Senator Robert Menendez (D-NJ), Chairman of the Subcommittee on Housing, Transportation and Community Development, called the bipartisan legislation “an incredible boon to New Jersey.”  By cutting waste and eliminating earmarks, the bill will provide New Jersey $519 million in federal transit funding, an increase of over $63 million per year.  If passed by the full Senate and House, New Jersey would receive more federal transit funding per year than ever before -- without increased overall federal spending.\n",
      "\n",
      "“For over two years now, I have worked across party lines to help craft a bill that will invest in our infrastructure and improve public transportation for New Jersey families and commuters,”“This bill provides our state more federal transit funding than it has ever received before.   What does more funding mean?  It means more resources to protect and create good paying jobs and more funds to make the improvements we need to ease congestion and delays.”    said Menendez. \n",
      "\n",
      "Menendez also applauded the inclusion of a number of provisions he championed including:\n",
      "A New $20 Million Transit Oriented Development Planning Program.  The $20 million program will help communities create more livable communities by planning new development around new transit hubs.  The provision is based on similar language in Senator Menendez’s Livable communities Act, and in New Jersey would work in tandem with the state’s Transit Village program.\n",
      "Increased Funding for the National Transit Institute at Rutgers (NTI) [$5million]. NTI provides training, education, and clearinghouse services in support of public transportation and quality of life for the entire nation.  In recent years this important national program has seen its funding slashed, despite the increased need for training in the face of an ongoing wave of retirements in the industry. This bill will raise NTI’s funding to $5 million per year from $3.8 million.\n",
      "Increased Clean Fuels Program Funding [From $51.5M to $65M].  This competitive program for clean fuel transit vehicles and for refueling infrastructure will help agencies switch from dirty, expensive fuels, to cleaner, cheaper fuels.  This will help improve air quality and allow transit agencies to untether themselves from volatile oil prices.  \n",
      "Increased Funding for Transportation for Seniors and the Disabled [NJ Funding Goes From $6.5 M to $7.8 M]. With demand for senior transportation increasing, the bill is able to meet that demand with increased resources.\n",
      "Streamlined and Reformed “New Starts” Process. The bill streamlines the process for the federal approval of new projects and allows projects designed to increase capacity on existing systems rather than just allow new systems or new lines.  Older systems such as New Jersey’s that are at capacity could, for instance, use the program to add a new station, add another track, or purchase bigger train cars. \n",
      "###\n"
     ]
    }
   ],
   "source": [
    "print(pr.text[closest[2][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menendez: Bipartisan Senate Transit Bill Will Deliver NJ over $62 Million More Per Year\n",
      "                    \n",
      "                            Chair of Banking’s Transit Subcommittee Delivers Urgently Needed Transit Funding in a Flat Funded Bill\n",
      "                    \n",
      "                      January 30, 2012\n",
      "                     WASHINGTON - Senator Robert Menendez (D-NJ) today announced that Senate Banking Committee’s transit reauthorization mark, which he helped craft, will be a boon to New Jersey.  By cutting waste and eliminating earmarks, the bill will provide New Jersey $519 million in federal transit funding, an increase of over $62 million per year.  If passed, New Jersey would receive more federal transit funding per year than ever before and without increased overall federal spending.\n",
      "\n",
      "“For over two years now, we have worked to craft a bill to improve public transportation in New Jersey and across America,” said Menendez, who worked closely with former Chairman Dodd, Chairman Johnson, and Ranking Member Shelby on the bill.  “By wringing out waste and earmarks, the bill provides New Jersey with more federal transit funding than it has ever received before, and does so when NJ Transit desperately needs money to address delays and rider complaints.” \n",
      "\n",
      "The bill also contains a number of programs the Senator championed including:\n",
      "A New $20 Million Transit Oriented Development Planning Program.  The $20 million program will help communities create more livable communities by planning new development around new transit hubs.  The provision is based on similar language in Senator Menendez’s Livable communities Act, and in New Jersey would work in tandem with the state’s Transit Village program.\n",
      "Increased Funding for the National Transit Institute at Rutgers (NTI) [$5million]. NTI provides training, education, and clearinghouse services in support of public transportation and quality of life for the entire nation.  In recent years this important national program has seen its funding slashed, despite the increased need for training in the face of an ongoing wave of retirements in the industry. This bill will raise NTI’s funding to $5 million per year from $3.8 million.\n",
      "Increased Clean Fuels Program Funding [From $51.5M to $65M].  This competitive program for clean fuel transit vehicles and for refueling infrastructure will help agencies switch from dirty, expensive fuels, to cleaner, cheaper fuels.  This will help improve air quality and allow transit agencies to untether themselves from volatile oil prices.  \n",
      "Increased Funding for Transportation for Seniors and the Disabled [NJ Funding Goes From $6.5 M to $7.8 M]. With demand for senior transportation increasing, the bill is able to meet that demand with increased resources.\n",
      "Streamlined and Reformed “New Starts” Process. The bill streamlines the process for the federal approval of new projects and allows projects designed to increase capacity on existing systems rather than just allow new systems or new lines.  Older systems such as New Jersey’s that are at capacity could, for instance, use the program to add a new station, add another track, or purchase bigger train cars. \n",
      "\n",
      " “With gasoline prices expected to spike again this summer, we need to invest in public transportation more than ever,” said Menendez.  “This bill stretches the federal dollar further and puts more money directly into our transit systems.  Hopefully, we will pass this bill and provide transit agencies with the resources they need to manage projected ridership increases this summer.”\n",
      " ###\n"
     ]
    }
   ],
   "source": [
    "print(pr.text[closest[3][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are all on the subject of transit funding!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
